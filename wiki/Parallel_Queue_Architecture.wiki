#summary Design and Implementation of Parallel Queues using ParallelStore

= Introduction =

== Overview ==

Tungsten Replicator uses _parallel stores_ to implement pipeline stages in which multiple tasks read and apply transactions concurrently.  The most significant application of this technique is known as _parallel apply_, which speeds up replication by executing slave updates on different shards in parallel.  Parallel stores split serialized transactions into multiple output queues, which is why we sometimes call them _parallel queues_.  

This wiki page summarizes the current parallel store implementations.  It is not user documentation but rather describes the algorithms behind the code as it currently stands and illustrates the trade-offs between different parallel store approaches.  The goal is to provide orientation when reading the code, designing tests, and developing new applications for parallel stores. 

== Caveats ==

The Tungsten solution to parallel replication has developed incrementally over a period of a couple of years.  Our understanding of the problem and solutions has evolved considerably during this time.  This means that while the overall model and general algorithms are fairly consistent, some of the interfaces and naming for individual classes are not.  There will consequently need to be some clean-up over time to rationalize naming and restructure interfaces.  

== Additional References ==

For more information on the overall approach to parallel replication in Tungsten, see the following: 

  * [http://www.continuent.com/downloads/documentation Continuent Product Documentation]

  * [http://scale-out-blog.blogspot.com/2010/10/parallel-replication-on-mysql-report.html Parallel Replication on MySQL: Report from the Trenches]

  * [http://scale-out-blog.blogspot.com/2011/03/parallel-replication-using-shards-is.html Parallelization Using Shards Is the Only Workable Approach for SQL]

= What We Mean by a Tungsten Parallel Store = 

Tungsten Replicator uses a pipeline-based execution model, where pipelines are event flows that extract from a source and apply to a sink at the other end.  Pipelines consist of one or more stages, which implement extract-filter-apply loops to transfer events.  Stores sit between stages and buffer events as they pass through the pipeline.  Stores therefore act as queues within the pipeline and may be either persistent (like the THL) or non-persistent in-memory buffers. 

A Tungsten parallel store is a type of [http://code.google.com/p/tungsten-replicator/source/browse/trunk/replicator/src/java/com/continuent/tungsten/replicator/storage/Store.java Store] that demultiplexes a serialized store of events into multiple queues.  Parallel stores implement an additional [http://code.google.com/p/tungsten-replicator/source/browse/trunk/replicator/src/java/com/continuent/tungsten/replicator/storage/ParallelStore.java ParallelStore] interface that provides control methods to enable safe shutdown.  

The following diagrams shows how parallel stores work in general.  A single stage task delivers serialized events to the store.  The store demultiplexes the events into parallel queues that feed multiple tasks in the next stage.  

[https://s3.amazonaws.com/extras.continuent.com/code.google.com-images/Tungsten-Parallel-Queue-Model.jpg]

Parallel threads are referred to as _channels_ in user documentation.  Channels are implemented so far as queues within the parallel store that serve corresponding tasks in the stage that extracts from the store.  Extractors on the parallel store therefore have an additional method documented in interface [http://code.google.com/p/tungsten-replicator/source/browse/trunk/replicator/src/java/com/continuent/tungsten/replicator/extractor/ParallelExtractor.java ParallelExtractor] to set the task ID so that each task extracts from the correct queue.  

ParallelStore implementations must deal with a number of practical issues that make the implementations a little harder than one might think at first. 

  * Partitioning.  All current implementations call an instance of the [http://code.google.com/p/tungsten-replicator/source/browse/trunk/replicator/src/java/com/continuent/tungsten/replicator/storage/parallel/Partitioner.java Partitioner] interface to split events into separate channels.  This is not formally part of the model though it probably should be.  Partitioning algorithms must always be idempotent and return the same channel assignment for a given number of channels.  
  * Full serialization.  Some transactions cannot safely run in parallel.  Internally we call these critical events.  Parallel stores must be able to revert to fully serialized operation when a critical event appears. 
  * Restart.  Tungsten marks the current seqno of each apply task (usually in the trep_commit_seqno table for SQL databases).  For single-threaded tasks, this is easy because there is a constant stream of events so we update the restart position at the end of each block.  However, with parallel operation we may find that some queues are empty for prolonged periods of time and do not mark position.  This causes a problem when a replicator crashes or is terminated, because the applier restarts at the least advanced position.  In pathological cases this can cause replicators to rescan millions of transactions to find the restart position.  Current implementations solve this by periodically inserting synthetic control events that cause the down streams tasks to update their position.  
  * Clean shutdown.  Parallel stores need to be able to bring all channels to the same point and shut down.  This allows users to change the number of channels or revert back to non-parallel operation safely We currently handle this by inserting a stop control event on all channels.  When the downstream stage tasks receive this event they shut down.  Clean shutdown is complete when all stage tasks have shut down in this way. 

There are many ways to implement this model in practice.  The following sections describe the current implementations. 

= In-Memory Parallel Store =

== [http://code.google.com/p/tungsten-replicator/source/browse/trunk/replicator/src/java/com/continuent/tungsten/replicator/storage/parallel/ParallelQueueStore.java ParallelQueueStore] ==

=== Architecture ===

This implemention works by splitting applied events into a separate queue for each channel.  One task thread applies new events.  A task thread in the succeeding stage extracts from each individual queue.  The general flow is illustrated in the following diagram. 

[https://s3.amazonaws.com/extras.continuent.com/code.google.com-images/Tungsten-ParallelQueueStore.jpg]

=== Implementation Notes ===

The implementation partitions events into channels using a partitioner, as described in the general discussion of parallel stores.  All processing including putting events into queues and fetching them out again occurs in the task threads, which means that no additional threads are required within the store itself.  

Queues are implemented using Java blocking queues, e.g., implementations of java.util.concurrency.BlockingQueue.  Each queue has a maximum size (maxSize property) and blocks when it is reached.  Blocking queues have the advantage that queue methods synchronize on the queue, which ensures that data are properly visible across queue reader and writer threads in accordance with guarantees of visibility provided by Java locks.  

=== Advantages ===

The ParallelQueueStore has a number of advantages. 

  # There is no latency between reading and writing, since everything is in-memory. 
  # It is relatively simple to implement correct serialization and clean shutdown in this model.  
  # The fact that no extra threads are necessary eliminates coordination complexity within the store and reduces the number of cases that need to be tested to confirm correct operation. 
  # There is no persistent state to manage if we crash and restart. 

=== Disadvantages ===

On the other hand there are problems with the in-memory design that make it difficult to use on some workloads.  The main reason is that even on systems with "nice" shard partitioning patterns across channels, events tend to show up in groups in different channels, so that at any given instant it is common to have many events on some channels and few or none on the rest.  This leads in turn to the following problems. 

  # You need to allocate enormous queue sizes to get good serialization.  Otherwise, one busy channel will fill up the overall parallel queue capacity, effectively serializing everything.  This turns the replicator into a bloated memory hog.  It is especially problematic with row replication, which generates an update for every row that changes. 
  # Even with large queues you still get serialization problems.  It is not uncommon for some SQL statements to execute very slowly, for example following a restart when DBMS caches are cold.  Consequently, you may again find that all channels block behind one or two slow statements.  

Systems that process large numbers of transactions per second tend to see these problems in spades.  On a system that processes 1000/TPS 60 seconds of processing is 60K transactions, which in turn could be 200GB of memory in the Java heap.  This kind of math quickly leads to very large memory allocations to prevent serialization. 

= Disk-Based Parallel Store =

[https://s3.amazonaws.com/extras.continuent.com/code.google.com-images/Tungsten-THLParallelQueue.jpg]